{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UWiPWcWBYbB9"
      },
      "source": [
        "# Feature Extraction using VGG\n",
        "\n",
        "One way to extract features is by using CNNs. For this purpose we will use VGG16.\n",
        "\n",
        "The CNN should be used as fixed feature extractor on a new task. "
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A1lQNfBpXt6I"
      },
      "source": [
        "## Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "VT2vh22hXsig"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.applications.vgg16 import VGG16\n",
        "from tensorflow.keras.preprocessing import image\n",
        "from tensorflow.keras.applications.vgg16 import preprocess_input\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "from keras.layers import *\n",
        "import keras"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "qrom-HX4fuxI"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import matplotlib.image as mpimg\n",
        "\n",
        "import cv2\n",
        "import os\n",
        "\n",
        "import itertools"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sk_HXWV5aA0D"
      },
      "source": [
        "## Dataset\n",
        "\n",
        "The dataset is taken by Kaggle at the following link: https://www.kaggle.com/slothkong/10-monkey-species .\\\n",
        "It's called \"10 Monkey Species\" and as the name suggest the task is to classify 10 species of monkey from images. \\\n",
        "The dataset is balanced. \\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NkTin8EMatQP",
        "outputId": "d25c9e5b-0825-4d6f-cb59-30bc63f18c05"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive', force_remount=True)\n",
        "root_dir = '/content/drive/MyDrive'\n",
        "base_dir = root_dir + '/AML - Assigment 4/10 Monkey Species'\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "Z0odb7YkbdRx"
      },
      "outputs": [],
      "source": [
        "train_path = base_dir + '/training/'\n",
        "test_path = base_dir + '/validation/'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sXncVaPCskqU"
      },
      "source": [
        "Now we just try to read an image to see how the images are."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "id": "6Dp7vCdZlHSF"
      },
      "outputs": [],
      "source": [
        "def load_images_from_folder(folder):\n",
        "    images = []\n",
        "    for filename in os.listdir(folder):\n",
        "        img = cv2.imread(os.path.join(folder,filename))\n",
        "        if img is not None:\n",
        "            images.append(img)\n",
        "    return images"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TaGQaBNesYgE"
      },
      "source": [
        "## Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 38,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "78epd8-YsX92",
        "outputId": "595bb930-503b-4f68-af12-b8ba01be8d7f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading data from https://storage.googleapis.com/tensorflow/keras-applications/vgg16/vgg16_weights_tf_dim_ordering_tf_kernels.h5\n",
            "553467904/553467096 [==============================] - 6s 0us/step\n",
            "553476096/553467096 [==============================] - 6s 0us/step\n",
            "Model: \"vgg16\"\n",
            "_________________________________________________________________\n",
            " Layer (type)                Output Shape              Param #   \n",
            "=================================================================\n",
            " input_1 (InputLayer)        [(None, 224, 224, 3)]     0         \n",
            "                                                                 \n",
            " block1_conv1 (Conv2D)       (None, 224, 224, 64)      1792      \n",
            "                                                                 \n",
            " block1_conv2 (Conv2D)       (None, 224, 224, 64)      36928     \n",
            "                                                                 \n",
            " block1_pool (MaxPooling2D)  (None, 112, 112, 64)      0         \n",
            "                                                                 \n",
            " block2_conv1 (Conv2D)       (None, 112, 112, 128)     73856     \n",
            "                                                                 \n",
            " block2_conv2 (Conv2D)       (None, 112, 112, 128)     147584    \n",
            "                                                                 \n",
            " block2_pool (MaxPooling2D)  (None, 56, 56, 128)       0         \n",
            "                                                                 \n",
            " block3_conv1 (Conv2D)       (None, 56, 56, 256)       295168    \n",
            "                                                                 \n",
            " block3_conv2 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_conv3 (Conv2D)       (None, 56, 56, 256)       590080    \n",
            "                                                                 \n",
            " block3_pool (MaxPooling2D)  (None, 28, 28, 256)       0         \n",
            "                                                                 \n",
            " block4_conv1 (Conv2D)       (None, 28, 28, 512)       1180160   \n",
            "                                                                 \n",
            " block4_conv2 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_conv3 (Conv2D)       (None, 28, 28, 512)       2359808   \n",
            "                                                                 \n",
            " block4_pool (MaxPooling2D)  (None, 14, 14, 512)       0         \n",
            "                                                                 \n",
            " block5_conv1 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv2 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_conv3 (Conv2D)       (None, 14, 14, 512)       2359808   \n",
            "                                                                 \n",
            " block5_pool (MaxPooling2D)  (None, 7, 7, 512)         0         \n",
            "                                                                 \n",
            " flatten (Flatten)           (None, 25088)             0         \n",
            "                                                                 \n",
            " fc1 (Dense)                 (None, 4096)              102764544 \n",
            "                                                                 \n",
            " fc2 (Dense)                 (None, 4096)              16781312  \n",
            "                                                                 \n",
            " predictions (Dense)         (None, 1000)              4097000   \n",
            "                                                                 \n",
            "=================================================================\n",
            "Total params: 138,357,544\n",
            "Trainable params: 138,357,544\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ]
        }
      ],
      "source": [
        "#This give us all the model but the last layer\n",
        "from tensorflow.keras.models import Model\n",
        "base_model = VGG16(weights='imagenet')\n",
        "base_model.summary()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "AA-i8qTSfS-t"
      },
      "source": [
        "If we apply multiple cut we have multiple features. \\\n",
        "We have to find where to cut to extract usefull information."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zorYgceixwR7"
      },
      "source": [
        "To better manage the RAM there are various options:\n",
        "If I make a network with input 24x24 and put 12x12 inputs may break the convolutionary layers because they are thought differently.\n",
        "\n",
        "1.   Resize in memory often can't be done\n",
        "2.   Upsample/Downsample before putting it in the NN\n",
        "3.   Upsample/Downsample as first layer of the NN (not trainable, best option).\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nfMBZgwuNhKL"
      },
      "source": [
        "Since my dataset is not big I can pre-process it witout problems. \\\n",
        "Anyway, I can't use a MaxPooling Layer because the inputs have different shapes. \\\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 39,
      "metadata": {
        "id": "FO6YZ8uGshz3"
      },
      "outputs": [],
      "source": [
        "model_1 = Model(inputs=base_model.input, outputs=base_model.get_layer('block4_pool').output) # block4_pool"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 40,
      "metadata": {
        "id": "EpWlEpW0tWjR"
      },
      "outputs": [],
      "source": [
        "model_2 = Model(inputs=base_model.input, outputs=base_model.get_layer('block5_pool').output)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 41,
      "metadata": {
        "id": "ruS7rPdEtbyg"
      },
      "outputs": [],
      "source": [
        "model_3 = Model(inputs=base_model.input, outputs=base_model.get_layer('fc2').output)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EeMw2cOX_NM1"
      },
      "source": [
        "I have chosen the layers so that they are more and more specific for the initial task. \\\n",
        "Unfortunately I was unable to get higher layer output due to colab related memory problems. \\\n",
        "However, I am convinced that layers higher up, but not too much, can help more."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L7aw-wI4Q1vx"
      },
      "source": [
        "### Model 1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 42,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_Pn4yFEUBWp1",
        "outputId": "67054cc5-fb62-458f-b1d9-9dcebe568884"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "272\n"
          ]
        }
      ],
      "source": [
        "test_features_1 = []\n",
        "for image in flatted_test:\n",
        "  test_features_1.append( model_1.predict(image) )\n",
        "\n",
        "print(len(test_features_1))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 43,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Q03DywVfLdnm",
        "outputId": "39f7f191-f726-468a-b31a-a773dba0f47b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1098\n"
          ]
        }
      ],
      "source": [
        "train_features_1 = []\n",
        "for image in flatted_train:\n",
        "  train_features_1.append( model_1.predict(image) )\n",
        "\n",
        "print(len(train_features_1))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NxUhqZ_GQ7U4"
      },
      "source": [
        "### Model 2"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 44,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mitloIajQ_Le",
        "outputId": "d2a7e5fb-4138-49c2-932e-32cc6f33d48d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "272\n"
          ]
        }
      ],
      "source": [
        "test_features_2 = []\n",
        "for image in flatted_test:\n",
        "  test_features_2.append( model_2.predict(image) )\n",
        "\n",
        "print(len(test_features_2))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 45,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Sjypk2DBRB2r",
        "outputId": "edef87e8-b142-496f-93ee-cfa4fb60bb98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1098\n"
          ]
        }
      ],
      "source": [
        "train_features_2 = []\n",
        "for image in flatted_train:\n",
        "  train_features_2.append( model_2.predict(image) )\n",
        "\n",
        "print(len(train_features_2))"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UmTHYTdUQ8ME"
      },
      "source": [
        "### Model 3"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 46,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YepWJWqORGVA",
        "outputId": "85f4d74b-5be4-4087-eb9b-1557ad8c6fe1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "272\n"
          ]
        }
      ],
      "source": [
        "test_features_3 = []\n",
        "for image in flatted_test:\n",
        "  test_features_3.append( model_3.predict(image) )\n",
        "\n",
        "print(len(test_features_3))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 47,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B1FnOsQmRI_m",
        "outputId": "4084f937-9e76-474e-be87-853a85538e51"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "1098\n"
          ]
        }
      ],
      "source": [
        "train_features_3 = []\n",
        "for image in flatted_train:\n",
        "  train_features_3.append( model_3.predict(image) )\n",
        "\n",
        "print(len(train_features_3))"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "Assigment4.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
